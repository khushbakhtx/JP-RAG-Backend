{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Med RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI, ML, and Python\\RAG-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data['articles']\n",
    "\n",
    "articles = load_articles('../../data/raw/articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [Document(page_content=article['content'], metadata=article.get('metadata', {})) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    "    )\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=embedding\n",
    "    )\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY-BERT key word search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "keybert_model = KeyBERT()\n",
    "\n",
    "def extract_keywords_from_text(text, top_n=5):\n",
    "    keywords = keybert_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=top_n)\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "def search_using_keywords(user_query, k_candidates=3):\n",
    "    keywords = extract_keywords_from_text(user_query)\n",
    "    \n",
    "    relevant_docs = []\n",
    "    for doc in splits:\n",
    "        doc_keywords = extract_keywords_from_text(doc.page_content)\n",
    "        if any(keyword in doc_keywords for keyword in keywords):\n",
    "            relevant_docs.append(doc)\n",
    "\n",
    "    return relevant_docs[:k_candidates]\n",
    "\n",
    "k_candidates = 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, api_key, model_name=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api-inference.huggingface.co/v1/\",\n",
    "            api_key=api_key\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def get_response_from_model(self, context, user_query):\n",
    "        prompt_text = f\"\"\"\n",
    "        Answer the following question based on the context provided:\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: \n",
    "        {user_query}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model_name, \n",
    "            messages=messages, \n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\"  \n",
    "llm_client = LLMClient(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(user_query):\n",
    "    retrieved_docs = search_using_keywords(user_query, k_candidates) \n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    response = llm_client.get_response_from_model(context, user_query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from RAG: There is not enough context provided to determine the specific area or topic you are referring to. The phrase \"how many rules there are\" could be related to various fields such as:\n",
      "\n",
      "- Laws and regulations\n",
      "- Sports\n",
      "- Traffic rules\n",
      "- Social norms\n",
      "- Game rules\n",
      "\n",
      "Without more context, it's difficult to give a specific answer. If you could provide more information about the topic, I'd be happy to try and help.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Why does Peterson believe that opportunities often lie in places others avoid?\"\n",
    "response = rag_system(user_query)\n",
    "\n",
    "print(\"Response from RAG:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
